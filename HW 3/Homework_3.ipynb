{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccD5Kq24WAN3"
   },
   "source": [
    "<div style=\"text-align: right\">\n",
    "    <i>\n",
    "        LIN 5300<br>\n",
    "        Spring 2023 <br>\n",
    "        Jon Rawski <br>\n",
    "    </i>\n",
    "</div>\n",
    "\n",
    "\n",
    "# Submission Guidelines\n",
    "Please upload your completed notebook file on Canvas in the following format:\n",
    "\n",
    "LastName_FirstName_HW3.ipynb\n",
    "\n",
    "# Preliminaries (2 points)\n",
    "\n",
    "1. Load in all of the packages you will need for this assignment in the cell below. \n",
    "\n",
    "  If you load in other packages later in the notebook, be sure to bring them up here. This is good coding practice and will look cleaner for everyone when reading your code.\n",
    "\n",
    "  You will need the following:\n",
    "\n",
    "  * To load a plain text file in with the colab interface (by uploading the file to the notebook)\n",
    "  * To count occurrances of tokens\n",
    "  * A regular expression tokenizer\n",
    "  * The NLTK tokenizer for English\n",
    "  * The spaCy word tokenizer for English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OfOQBQztFnqW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/henrypham/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in packages that you will use in this notebook\n",
    "from pprint import pprint\n",
    "# from google.colab import drive //doing work in vscode\n",
    "from collections import Counter\n",
    "import spacy \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# put other packages you will use below this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZSPSo-TWTxL"
   },
   "source": [
    "# Preliminaries 2 (1 point)\n",
    "\n",
    "Load in the file called `faustus.txt` in the variable `faust`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bl225PcgV61a"
   },
   "outputs": [],
   "source": [
    "#Import the file here\n",
    "faustus = open('faustus_clean.txt', 'r', encoding='utf-8').read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRL5OkT4Wn-4"
   },
   "source": [
    "# Question 1: 2 points\n",
    "\n",
    "In this section, we will be comparing different preprocessing strategies. \n",
    "\n",
    "First, tokenize the corpus using a simple regEx technique that relies on whitespaces to distinguish words (You can use the function we defined in class)!\n",
    "\n",
    "Save the resulting list into the variable `faust_regEx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nq6-78b1IGPm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'marching', 'in', 'the', 'fields', 'of', 'thrasymene', 'where', 'mars', 'did']\n"
     ]
    }
   ],
   "source": [
    "# define a tokenize function using regex based on whitespace\n",
    "def regex_tokenizer(text):\n",
    "    return re.findall(r'\\w+', text)\n",
    "\n",
    "# define a variable for each token list\n",
    "faust_regEx = regex_tokenizer(faustus)\n",
    "\n",
    "# print the first 10 elements as a safety check\n",
    "print(faust_regEx[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UqHOFeVIGzB"
   },
   "source": [
    "# Question 2: 5 points\n",
    "\n",
    "Now, we are going to use the [`nltk` `word_tokenize`](https://www.kite.com/python/docs/nltk.word_tokenizefunction). You should have loaded nltk above in the very first block. Use `word_tokenize` on the corpus in a list of string arrays called `faust_nltk_tokenized`. Use a slice to print the fifth to the tenth elements of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "z8oOzoHiIzdh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'marching', 'in', 'the', 'fields', 'of', 'thrasymene', ',', 'where', 'mars']\n"
     ]
    }
   ],
   "source": [
    "# use nltk's word_tokenize function\n",
    "# save the output into a variable\n",
    "faust_nltk_tokenized = nltk.word_tokenize(faustus, \"english\", False)\n",
    "\n",
    "# print the first 10 elements as a safety check\n",
    "print(faust_nltk_tokenized[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PpfhdWSI0Me"
   },
   "source": [
    "# Question 3: 5 points\n",
    "\n",
    "Now, we are going to use the [`spacy`](https://spacy.io/usage/spacy-101) tokenization [function](https://spacy.io/api/tokenizer). \n",
    "\n",
    "The output that spacy gives you is more complicated than the output of `nltk`'s `word_tokenize` function, because the `spacy` API takes a string (e.g., \"I like cheese\") and returns a `doc` object. Within the `doc` object there are `token`s, and each `token` has a `text` object. \n",
    "\n",
    "For this question, what you need to do is:\n",
    "\n",
    "* load the spacy tokenizer with \"en\" as the default value\n",
    "* obtain the `doc` object by using the tokenizer over the list `faust_full` \n",
    "* instantiate an empty list `faust_spacy_tokenized` and implement a for loop through all of the  `token`s of the obtained doc, storing for each the `token.text` string object into your list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "mTwCwQ7nM8dX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'marching', 'in', 'the', 'fields', 'of', 'thrasymene', ',', 'where', 'mars']\n"
     ]
    }
   ],
   "source": [
    "# use spacy's tokenization features\n",
    "# had to run python -m spacy download en_core_web_sm in terminal to get this to work\n",
    "nlp = spacy.load(name='en_core_web_sm')\n",
    "faustus_spacy = faustus.replace('\\n', '') #filter out '\\n' from the text\n",
    "doc = nlp(faustus_spacy)\n",
    "\n",
    "# save the output into a variable\n",
    "faust_spacy_tokenized = [token.text for token in doc]\n",
    "\n",
    "# print the first 10 elements as a safety check\n",
    "# initial run: had '\\n' in output, so I added a filter to remove it ^\n",
    "print(faust_spacy_tokenized[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLC2daI-Myn1"
   },
   "source": [
    "# Question 4: Compare tokenizations (5 points)\n",
    "\n",
    "Now that we have three tokenizations, we want to compare how similar the tokenizations are. \n",
    "\n",
    "In the code cell below, Visualize a reasonaly large subset of the corpus, tokenized under each of the three approaches above.\n",
    "Then, in the cell below that, explain how the outputs of these tokenizations differ, and what you can infer from the output about the ways the tokenization techniques differ (If necessary, you can modify the number of tokens you visualize until you converge on a good sample for the comparison). \n",
    "\n",
    "What do you think are the strengths and weaknesses of each tokenization approach? Do you think one of the tokenizations is better than another? Can you think of a way you would test which one is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sZ7km6oRYZs"
   },
   "source": [
    "### Question 4A: Code (2/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ld_dCKjLQaqf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegEx Tokenizer Output:\t ['not', 'marching', 'in', 'the', 'fields', 'of', 'thrasymene', 'where', 'mars', 'did', 'mate', 'the', 'warlike', 'carthagens', 'nor', 'sporting', 'in', 'the', 'dalliance', 'of', 'love', 'in', 'courts', 'of', 'kings', 'where', 'state', 'is', 'overturn', 'd', 'nor', 'in', 'the', 'pomp', 'of', 'proud', 'audacious', 'deeds', 'intends', 'our', 'muse', 'to', 'vaunt', 'her', 'heavenly', 'verse', 'only', 'this', 'gentles', 'we']\n",
      "\n",
      "NLTK Tokenizer Output:\t ['not', 'marching', 'in', 'the', 'fields', 'of', 'thrasymene', ',', 'where', 'mars', 'did', 'mate', 'the', 'warlike', 'carthagens', ';', 'nor', 'sporting', 'in', 'the', 'dalliance', 'of', 'love', ',', 'in', 'courts', 'of', 'kings', 'where', 'state', 'is', 'overturn', \"'d\", ';', 'nor', 'in', 'the', 'pomp', 'of', 'proud', 'audacious', 'deeds', ',', 'intends', 'our', 'muse', 'to', 'vaunt', 'her', 'heavenly']\n",
      "\n",
      "SpaCy Tokenizer Output:\t ['not', 'marching', 'in', 'the', 'fields', 'of', 'thrasymene', ',', '\\n', 'where', 'mars', 'did', 'mate', 'the', 'warlike', 'carthagens', ';', '\\n', 'nor', 'sporting', 'in', 'the', 'dalliance', 'of', 'love', ',', '\\n', 'in', 'courts', 'of', 'kings', 'where', 'state', 'is', \"overturn'd\", ';', '\\n', 'nor', 'in', 'the', 'pomp', 'of', 'proud', 'audacious', 'deeds', ',', '\\n', 'intends', 'our', 'muse']\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "# take a large subset of the corpus from faustus_clean.txt\n",
    "faust_subset = faustus[:100000]\n",
    "#tokenize under the 3 appraoches done before\n",
    "\n",
    "faust_regEx_subset = regex_tokenizer(faust_subset) \n",
    "\n",
    "faust_nltk_subset = nltk.word_tokenize(faust_subset, \"english\", False)\n",
    "\n",
    "faust_spacy = faust_subset.replace('\\n', '')\n",
    "doc_subset = nlp(faust_spacy)\n",
    "faust_spacy_subset = [token.text for token in doc_subset]\n",
    "\n",
    "print(\"RegEx Tokenizer Output:\\t\", faust_regEx_subset[:50])\n",
    "print(\"\\nNLTK Tokenizer Output:\\t\", faust_nltk_subset[:50])\n",
    "print(\"\\nSpaCy Tokenizer Output:\\t\", faust_spacy_subset[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CbwIX8AQm9Y"
   },
   "source": [
    "### Question 4B: Free response (3/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCVSKvDZGUDS"
   },
   "source": [
    "**ANSWER:** \n",
    "\n",
    "The three tokenizers are all unique in complexity and accuracy. The regex tokenizer is the simplest to use, using patterns to choose words but struggling with contractions like \"don't.\" NLTK accomplishes this better by keeping contractions along with punctuation, offering a great mix of speed and accuracy. SpaCy is the most complicated, discovering named entities and context but taking longer due to its complexity. Token quantity comparison, contraction management, and performance on real-world text are great things to consider when comparing approaches. Regex is appropriate for small tasks, NLTK everyday tasks, and spaCy high accuracy tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlivQPJXf6m2"
   },
   "source": [
    "# Question 5: Tabulating word counts under different algorithms (10 points)\n",
    "\n",
    "Now that you have compared and contrasted different tokenization algorithms, consider the effect that tokenization can have on our ability to characterize a corpus as a whole. \n",
    "\n",
    "Load in the `Counter` module and extract counts of all of the words under each of the three tokenizations schemes. Look at the top 5 most frequent (using the `.most_frequent()` method) and the top 10 least frequent (hint: use negative indices) words. In our data, what appear to be the biggest sources of disagreement? Do these confirm or disconfirm your hypotheses in the previous question? How or how not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjwPTSJA_txp"
   },
   "source": [
    "### Question 5A: Code (5/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aiXEsEaVdJTB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 most frequent words (RegEx): [('and', 509), ('the', 501), ('i', 400), ('of', 338), ('to', 333)]\n",
      "Top 5 most frequent words (NLTK): [(',', 1962), ('.', 562), ('and', 504), ('the', 500), ('i', 396)]\n",
      "Top 5 most frequent words (SpaCy): [(',', 1949), ('the', 468), ('and', 449), ('of', 326), ('i', 323)]\n",
      "\n",
      "Top 10 least frequent words (RegEx): [('unlawful', 1), ('deepness', 1), ('entice', 1), ('practise', 1), ('permits', 1), ('terminat', 2), ('hora', 1), ('diem', 1), ('auctor', 1), ('opus', 1)]\n",
      "Top 10 least frequent words (NLTK): [('unlawful', 1), ('deepness', 1), ('entice', 1), ('practise', 1), ('permits', 1), ('terminat', 2), ('hora', 1), ('diem', 1), ('auctor', 1), ('opus', 1)]\n",
      "Top 10 least frequent words (SpaCy): [('deepness', 1), ('entice', 1), ('witsto', 1), ('practise', 1), ('permits.terminat', 1), ('hora', 1), ('diem', 1), ('terminat', 1), ('auctor', 1), ('opus', 1)]\n"
     ]
    }
   ],
   "source": [
    "faust_regEx_counts = Counter(faust_regEx)\n",
    "faust_nltk_counts = Counter(faust_nltk_tokenized)\n",
    "faust_spacy_counts = Counter(faust_spacy_tokenized)\n",
    "\n",
    "print(\"Top 5 most frequent words (RegEx):\", faust_regEx_counts.most_common(5))\n",
    "print(\"Top 5 most frequent words (NLTK):\", faust_nltk_counts.most_common(5))\n",
    "print(\"Top 5 most frequent words (SpaCy):\", faust_spacy_counts.most_common(5))\n",
    "\n",
    "print(\"\\nTop 10 least frequent words (RegEx):\", list(faust_regEx_counts.items())[-10:])\n",
    "print(\"Top 10 least frequent words (NLTK):\", list(faust_nltk_counts.items())[-10:])\n",
    "print(\"Top 10 least frequent words (SpaCy):\", list(faust_spacy_counts.items())[-10:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bud1U9iM_vgh"
   },
   "source": [
    "### Question 5B: Free response (5/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTWirBBEGPjO"
   },
   "source": [
    "The biggest areas of disagreement between tokenizers are punctuation handling, contractions, and token merging. The RegEx tokenizer just ignores punctuation, so function words like \"and\" and \"the\" are the most common on its list. NLTK and spaCy, however, each handle punctuation as a separate token, increasing their counts and making words like \", \" (comma) the most common. In addition, spaCy will sometimes incorrectly merge words together, and \"permits.terminat\" and \"witsto\" are examples which do not exist anywhere else. These outputs confirm the initial hypothesis that RegEx is elegant but simplistic, NLTK is organized but readable, and spaCy is complex but overcompensates under certain circumstances. The discrepancies are evidence of how different tokenization choices will have some effect on downstream NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FvuYyre5F_P"
   },
   "source": [
    "# Question 6: Tabulating pointwise mutual information (15 points)\n",
    "\n",
    "Mutual information is a computation that is very similar to computing a conditional probability. Recall that computing a conditional probability, defined below, requires knowing two probabilities. The first, $p(A \\cap B)$, is the probability of observing $A$ and $B$ at the same time (so observing the bigram as a type). The second, $p(A)$, is the probability of observing $A$ across all contexts.\n",
    "\n",
    "Recall that we have used MLE so to approximate all of these by their frequencies in a corpus. For example, $p(A)$ can be approximated by:\n",
    "\n",
    "<center> $ \\large p(A) \\approx \\frac{count(A)}{\\sum_{w \\in V}count(w)} $ </center>\n",
    "\n",
    "Mutual information is very similar, but requires dividing the co-occurence statistic by two probabilities $p(A)$ and $p(B)$.\n",
    "\n",
    "\n",
    "<center>$ \\large MI = \\frac{p(A \\cap B)}{p(A) \\cdot p(B)} $</center>\n",
    "\n",
    "<hr />\n",
    "\n",
    "\n",
    "This question contains multiple parts to respond to.\n",
    "\n",
    "1. Compute the bigram frequencies of all words in our `faustus` corpus. You may use whatever tokenization scheme you think performs the best.\n",
    "2. For each of the bigrams in that abstracts, compute the mutual information of that bigram\n",
    "3. Pick a subset of bigrams and and print their mutual information value to the notebook.\n",
    "4. Answer the questions in the free response section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pXgmepfDDAm"
   },
   "source": [
    "### Question 6A: Computing mutual information for bigrams in one sentence (10/15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "PXPXWtAfCRGy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram: ('not', 'marching'), Mutual Information: 7.106723820851059\n",
      "Bigram: ('marching', 'in'), Mutual Information: 6.619608643528047\n",
      "Bigram: ('in', 'the'), Mutual Information: 1.749243923944643\n",
      "Bigram: ('the', 'fields'), Mutual Information: 5.167096438830541\n",
      "Bigram: ('fields', 'of'), Mutual Information: 5.688733004182867\n",
      "Bigram: ('of', 'thrasymene'), Mutual Information: 5.688733004182867\n",
      "Bigram: ('thrasymene', ','), Mutual Information: 3.1089427831560554\n",
      "Bigram: (',', 'where'), Mutual Information: 2.1089427831560554\n",
      "Bigram: ('where', 'mars'), Mutual Information: 8.578029539776647\n",
      "Bigram: ('mars', 'did'), Mutual Information: 8.949998317163606\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "from math import log2\n",
    "\n",
    "## your code for Question 6 goes here\n",
    "# Function to generate bigrams\n",
    "def generate_bigrams(tokens):\n",
    "    return [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "\n",
    "# Tokenize the corpus using spaCy (chosen as the best tokenizer)\n",
    "tokens = faust_spacy_tokenized\n",
    "\n",
    "# Generate bigrams\n",
    "bigrams = generate_bigrams(tokens)\n",
    "\n",
    "# Count bigram frequencies\n",
    "bigram_counts = Counter(bigrams)\n",
    "\n",
    "# Count unigram frequencies\n",
    "unigram_counts = Counter(tokens)\n",
    "\n",
    "# Total number of tokens\n",
    "total_tokens = sum(unigram_counts.values())\n",
    "\n",
    "# Compute mutual information for each bigram\n",
    "bigram_mi = {}\n",
    "for bigram, bigram_count in bigram_counts.items():\n",
    "    p_bigram = bigram_count / total_tokens\n",
    "    p_word1 = unigram_counts[bigram[0]] / total_tokens\n",
    "    p_word2 = unigram_counts[bigram[1]] / total_tokens\n",
    "    mi = log2(p_bigram / (p_word1 * p_word2))\n",
    "    bigram_mi[bigram] = mi\n",
    "\n",
    "# Select a subset of bigrams to display\n",
    "subset_bigrams = list(islice(bigram_mi.items(), 10))\n",
    "\n",
    "# Print mutual information values for the subset\n",
    "for bigram, mi in subset_bigrams:\n",
    "    print(f\"Bigram: {bigram}, Mutual Information: {mi}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIEw6FcsdZ1f"
   },
   "source": [
    "### Question 6B: Free response (5/10 points)\n",
    "\n",
    "Characterize the different mutual information values of the subset you are looking at. What values are highest? What values are lowest? Do you think mutual information would be a better statistic to compute than a conditional probability? If so, why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzshSnbtGMCo"
   },
   "source": [
    "### <font color='red'>Your written response for question 6B goes here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCgDQrOaICRl"
   },
   "source": [
    "# Self Evaluation (5 points, added only if at least half of the previous exercises has been attempted)\n",
    "\n",
    "How do you think you did? Which parts did you struggle with the most? Which parts were easier than you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I did well in analyzing the tokenization methods and comparing their outputs. Identifying differences between regular expressions, NLTK, and spaCy was straightforward, especially when looking at how they handled punctuation and contractions. The hardest part was interpreting mutual information values and explaining why certain word pairs had high scores. Understanding the reasoning behind these relationships required more critical thinking than simply comparing token counts. On the other hand, identifying the biggest sources of disagreement between tokenizers was easier than expected because the patterns were clear in the output."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
