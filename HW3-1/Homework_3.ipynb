{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ccD5Kq24WAN3"
   },
   "source": [
    "<div style=\"text-align: right\">\n",
    "    <i>\n",
    "        LIN 5300<br>\n",
    "        Spring 2023 <br>\n",
    "        Jon Rawski <br>\n",
    "    </i>\n",
    "</div>\n",
    "\n",
    "\n",
    "# Submission Guidelines\n",
    "Please upload your completed notebook file on Canvas in the following format:\n",
    "\n",
    "LastName_FirstName_HW3.ipynb\n",
    "\n",
    "# Preliminaries (2 points)\n",
    "\n",
    "1. Load in all of the packages you will need for this assignment in the cell below. \n",
    "\n",
    "  If you load in other packages later in the notebook, be sure to bring them up here. This is good coding practice and will look cleaner for everyone when reading your code.\n",
    "\n",
    "  You will need the following:\n",
    "\n",
    "  * To load a plain text file in with the colab interface (by uploading the file to the notebook)\n",
    "  * To count occurrances of tokens\n",
    "  * A regular expression tokenizer\n",
    "  * The NLTK tokenizer for English\n",
    "  * The spaCy word tokenizer for English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OfOQBQztFnqW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Henry\n",
      "[nltk_data]     Pham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in packages that you will use in this notebook\n",
    "from pprint import pprint\n",
    "# from google.colab import drive //doing work in vscode\n",
    "from collections import Counter\n",
    "import spacy \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# put other packages you will use below this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZSPSo-TWTxL"
   },
   "source": [
    "# Preliminaries 2 (1 point)\n",
    "\n",
    "Load in the file called `faustus.txt` in the variable `faust`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bl225PcgV61a"
   },
   "outputs": [],
   "source": [
    "#Import the file here\n",
    "faustus = open('faustus_clean.txt', 'r', encoding='utf-8').read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRL5OkT4Wn-4"
   },
   "source": [
    "# Question 1: 2 points\n",
    "\n",
    "In this section, we will be comparing different preprocessing strategies. \n",
    "\n",
    "First, tokenize the corpus using a simple regEx technique that relies on whitespaces to distinguish words (You can use the function we defined in class)!\n",
    "\n",
    "Save the resulting list into the variable `faust_regEx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nq6-78b1IGPm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'marching', 'in', 'the', 'fields', 'of', 'thrasymene', 'where', 'mars', 'did']\n"
     ]
    }
   ],
   "source": [
    "# define a tokenize function using regex based on whitespace\n",
    "def regex_tokenizer(text):\n",
    "    return re.findall(r'\\w+', text)\n",
    "\n",
    "# define a variable for each token list\n",
    "faust_regEx = regex_tokenizer(faustus)\n",
    "\n",
    "# print the first 10 elements as a safety check\n",
    "print(faust_regEx[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UqHOFeVIGzB"
   },
   "source": [
    "# Question 2: 5 points\n",
    "\n",
    "Now, we are going to use the [`nltk` `word_tokenize`](https://www.kite.com/python/docs/nltk.word_tokenizefunction). You should have loaded nltk above in the very first block. Use `word_tokenize` on the corpus in a list of string arrays called `faust_nltk_tokenized`. Use a slice to print the fifth to the tenth elements of the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "z8oOzoHiIzdh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'marching', 'in', 'the', 'fields', 'of', 'thrasymene', ',', 'where', 'mars']\n"
     ]
    }
   ],
   "source": [
    "# use nltk's word_tokenize function\n",
    "# save the output into a variable\n",
    "faust_nltk_tokenized = nltk.word_tokenize(faustus, \"english\", False)\n",
    "\n",
    "# print the first 10 elements as a safety check\n",
    "print(faust_nltk_tokenized[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PpfhdWSI0Me"
   },
   "source": [
    "# Question 3: 5 points\n",
    "\n",
    "Now, we are going to use the [`spacy`](https://spacy.io/usage/spacy-101) tokenization [function](https://spacy.io/api/tokenizer). \n",
    "\n",
    "The output that spacy gives you is more complicated than the output of `nltk`'s `word_tokenize` function, because the `spacy` API takes a string (e.g., \"I like cheese\") and returns a `doc` object. Within the `doc` object there are `token`s, and each `token` has a `text` object. \n",
    "\n",
    "For this question, what you need to do is:\n",
    "\n",
    "* load the spacy tokenizer with \"en\" as the default value\n",
    "* obtain the `doc` object by using the tokenizer over the list `faust_full` \n",
    "* instantiate an empty list `faust_spacy_tokenized` and implement a for loop through all of the  `token`s of the obtained doc, storing for each the `token.text` string object into your list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mTwCwQ7nM8dX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'marching', 'in', 'the', 'fields', 'of', 'thrasymene', ',', 'where', 'mars']\n"
     ]
    }
   ],
   "source": [
    "# use spacy's tokenization features\n",
    "# had to run python -m spacy download en_core_web_sm in terminal to get this to work\n",
    "nlp = spacy.load(name='en_core_web_sm')\n",
    "faustus_spacy = faustus.replace('\\n', '') #filter out '\\n' from the text\n",
    "doc = nlp(faustus_spacy)\n",
    "\n",
    "# save the output into a variable\n",
    "faust_spacy_tokenized = [token.text for token in doc]\n",
    "\n",
    "# print the first 10 elements as a safety check\n",
    "# initial run: had '\\n' in output, so I added a filter to remove it ^\n",
    "print(faust_spacy_tokenized[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLC2daI-Myn1"
   },
   "source": [
    "# Question 4: Compare tokenizations (5 points)\n",
    "\n",
    "Now that we have three tokenizations, we want to compare how similar the tokenizations are. \n",
    "\n",
    "In the code cell below, Visualize a reasonaly large subset of the corpus, tokenized under each of the three approaches above.\n",
    "Then, in the cell below that, explain how the outputs of these tokenizations differ, and what you can infer from the output about the ways the tokenization techniques differ (If necessary, you can modify the number of tokens you visualize until you converge on a good sample for the comparison). \n",
    "\n",
    "What do you think are the strengths and weaknesses of each tokenization approach? Do you think one of the tokenizations is better than another? Can you think of a way you would test which one is better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4sZ7km6oRYZs"
   },
   "source": [
    "### Question 4A: Code (2/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ld_dCKjLQaqf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RegEx Tokenizer Output:\t ['not', 'marching', 'in', 'the', 'fields', 'of', 'thrasymene', 'where', 'mars', 'did', 'mate', 'the', 'warlike', 'carthagens', 'nor', 'sporting', 'in', 'the', 'dalliance', 'of', 'love', 'in', 'courts', 'of', 'kings', 'where', 'state', 'is', 'overturn', 'd', 'nor', 'in', 'the', 'pomp', 'of', 'proud', 'audacious', 'deeds', 'intends', 'our', 'muse', 'to', 'vaunt', 'her', 'heavenly', 'verse', 'only', 'this', 'gentles', 'we']\n",
      "\n",
      "NLTK Tokenizer Output:\t ['not', 'marching', 'in', 'the', 'fields', 'of', 'thrasymene', ',', 'where', 'mars', 'did', 'mate', 'the', 'warlike', 'carthagens', ';', 'nor', 'sporting', 'in', 'the', 'dalliance', 'of', 'love', ',', 'in', 'courts', 'of', 'kings', 'where', 'state', 'is', 'overturn', \"'d\", ';', 'nor', 'in', 'the', 'pomp', 'of', 'proud', 'audacious', 'deeds', ',', 'intends', 'our', 'muse', 'to', 'vaunt', 'her', 'heavenly']\n",
      "\n",
      "SpaCy Tokenizer Output:\t ['not', 'marching', 'in', 'the', 'fields', 'of', 'thrasymene', ',', 'where', 'mars', 'did', 'mate', 'the', 'warlike', 'carthagens', ';', 'nor', 'sporting', 'in', 'the', 'dalliance', 'of', 'love', ',', 'in', 'courts', 'of', 'kings', 'where', 'state', 'is', \"overturn'd;nor\", 'in', 'the', 'pomp', 'of', 'proud', 'audacious', 'deeds', ',', 'intends', 'our', 'muse', 'to', 'vaunt', 'her', 'heavenly', 'verse', ':', 'only']\n"
     ]
    }
   ],
   "source": [
    "# Put your code here\n",
    "# take a large subset of the corpus from faustus_clean.txt\n",
    "faust_subset = faustus[:100000]\n",
    "#tokenize under the 3 appraoches done before\n",
    "\n",
    "faust_regEx_subset = regex_tokenizer(faust_subset) \n",
    "\n",
    "faust_nltk_subset = nltk.word_tokenize(faust_subset, \"english\", False)\n",
    "\n",
    "faust_spacy = faust_subset.replace('\\n', '')\n",
    "doc_subset = nlp(faust_spacy)\n",
    "faust_spacy_subset = [token.text for token in doc_subset]\n",
    "\n",
    "print(\"RegEx Tokenizer Output:\\t\", faust_regEx_subset[:50])\n",
    "print(\"\\nNLTK Tokenizer Output:\\t\", faust_nltk_subset[:50])\n",
    "print(\"\\nSpaCy Tokenizer Output:\\t\", faust_spacy_subset[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0CbwIX8AQm9Y"
   },
   "source": [
    "### Question 4B: Free response (3/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCVSKvDZGUDS"
   },
   "source": [
    "The three methods of tokenization differ in handling punctuation, contractions, and structure. The **RegEx tokenizer** completely removes punctuation, splitting contractions like `\\\\\\\"overturn'd\\\\\\\"` into `\\\\\\\"overturn\\\\\\\"` and `\\\\\\\"d\\\\\\\"`, so it's fast but stupid. The **NLTK tokenizer** leaves punctuation in as individual tokens but nevertheless splits contractions in a capricious fashion, improving structure without deep linguistic awareness. The **spaCy tokenizer** is the most advanced, keeping punctuation and handling most contractions nicely, though it misinterprets `\\\\\\\"overturn'd;nor\\\\\\\"` by combining tokens inappropriately. **RegEx** is adequate for straightforward word counting, **NLTK** strikes the ideal balance of structure and simplicity, and **spaCy** provides the most natural-sounding sentence structure but overdoes it in complicated scenarios. It all depends on the NLP taskâ€”regex for basic word retrieval, NLTK for structured text, and spaCy for deep linguistic processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlivQPJXf6m2"
   },
   "source": [
    "# Question 5: Tabulating word counts under different algorithms (10 points)\n",
    "\n",
    "Now that you have compared and contrasted different tokenization algorithms, consider the effect that tokenization can have on our ability to characterize a corpus as a whole. \n",
    "\n",
    "Load in the `Counter` module and extract counts of all of the words under each of the three tokenizations schemes. Look at the top 5 most frequent (using the `.most_frequent()` method) and the top 10 least frequent (hint: use negative indices) words. In our data, what appear to be the biggest sources of disagreement? Do these confirm or disconfirm your hypotheses in the previous question? How or how not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjwPTSJA_txp"
   },
   "source": [
    "### Question 5A: Code (5/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "aiXEsEaVdJTB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Words:\n",
      "RegEx Tokenizer Output:\t [('and', 509), ('the', 501), ('i', 400), ('of', 338), ('to', 333)]\n",
      "\n",
      "NLTK Tokenizer Output:\t [(',', 1962), ('.', 562), ('and', 504), ('the', 500), ('i', 396)]\n",
      "\n",
      "SpaCy Tokenizer Output:\t [(',', 1949), ('the', 468), ('and', 449), ('of', 326), ('i', 323)]\n",
      "\n",
      "\n",
      "\n",
      "Least Common Words:\n",
      "RegEx Tokenizer Output:\t [('opus', 1), ('auctor', 1), ('diem', 1), ('hora', 1), ('permits', 1), ('practise', 1), ('entice', 1), ('deepness', 1), ('unlawful', 1), ('exhort', 1)]\n",
      "\n",
      "NLTK Tokenizer Output:\t [('opus', 1), ('auctor', 1), ('diem', 1), ('hora', 1), ('permits', 1), ('practise', 1), ('entice', 1), ('deepness', 1), ('unlawful', 1), ('exhort', 1)]\n",
      "\n",
      "SpaCy Tokenizer Output:\t [('opus', 1), ('auctor', 1), ('terminat', 1), ('diem', 1), ('hora', 1), ('permits.terminat', 1), ('practise', 1), ('witsto', 1), ('entice', 1), ('deepness', 1)]\n"
     ]
    }
   ],
   "source": [
    "## Your code for question 5A\n",
    "# Load in the `Counter` module and extract counts of all of the words under each of the three tokenizations schemes. Look at the top 5 most frequent (using the `.most_frequent()` method) and the top 10 least frequent (hint: use negative indices) words. In our data, what appear to be the biggest sources of disagreement? Do these confirm or disconfirm your hypotheses in the previous question? How or how not? \n",
    "\n",
    "# create a Counter object for each tokenized list\n",
    "faust_regEx_counter = Counter(faust_regEx_subset)\n",
    "faust_nltk_counter = Counter(faust_nltk_subset)\n",
    "faust_spacy_counter = Counter(faust_spacy_subset)\n",
    "\n",
    "# print the most common words for each tokenized list\n",
    "print(\"Most Common Words:\")\n",
    "print(\"RegEx Tokenizer Output:\\t\", faust_regEx_counter.most_common(5))\n",
    "print(\"\\nNLTK Tokenizer Output:\\t\", faust_nltk_counter.most_common(5))\n",
    "print(\"\\nSpaCy Tokenizer Output:\\t\", faust_spacy_counter.most_common(5))\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "# print the least common words for each tokenized list\n",
    "print(\"Least Common Words:\")\n",
    "print(\"RegEx Tokenizer Output:\\t\", faust_regEx_counter.most_common()[:-11:-1])\n",
    "print(\"\\nNLTK Tokenizer Output:\\t\", faust_nltk_counter.most_common()[:-11:-1])\n",
    "print(\"\\nSpaCy Tokenizer Output:\\t\", faust_spacy_counter.most_common()[:-11:-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bud1U9iM_vgh"
   },
   "source": [
    "### Question 5B: Free response (5/10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTWirBBEGPjO"
   },
   "source": [
    "The biggest sources of disagreement among tokenizers come from the treatment of punctuation, contractions, and token fusion. The **RegEx tokenizer** treats punctuation not at all, leading to function words like \"and\" and \"the\" dominating its frequency list. On the other hand, **NLTK** and **spaCy** treat punctuation as separate tokens, counting them twice and putting words like \", \" (comma) on top. In addition, **spaCy** at times inaccurately merges words, like `\\\\\\\"permits.terminat\\\"` and `\\\\\\\"witsto\\\"`, that never appear elsewhere. These outcomes **confirm** the original guess that **RegEx is elegant but simplistic, NLTK is a balance of structure and readability, and spaCy is complex but overcorrects in some cases**. The differences serve to demonstrate how different tokenization choices can profoundly impact downstream NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FvuYyre5F_P"
   },
   "source": [
    "# Question 6: Tabulating pointwise mutual information (15 points)\n",
    "\n",
    "Mutual information is a computation that is very similar to computing a conditional probability. Recall that computing a conditional probability, defined below, requires knowing two probabilities. The first, $p(A \\cap B)$, is the probability of observing $A$ and $B$ at the same time (so observing the bigram as a type). The second, $p(A)$, is the probability of observing $A$ across all contexts.\n",
    "\n",
    "Recall that we have used MLE so to approximate all of these by their frequencies in a corpus. For example, $p(A)$ can be approximated by:\n",
    "\n",
    "$\\large p(A) \\approx \\frac{count(A)}{\\sum_{w \\in V}count(w)}$\n",
    "\n",
    "Mutual information is very similar, but requires dividing the co-occurence statistic by two probabilities $p(A)$ and $p(B)$.\n",
    "\n",
    "\n",
    "$\\large MI = \\frac{p(A \\cap B)}{p(A) \\cdot p(B)}$\n",
    "\n",
    "<hr />\n",
    "\n",
    "\n",
    "This question contains multiple parts to respond to.\n",
    "\n",
    "1. Compute the bigram frequencies of all words in our `faustus` corpus. You may use whatever tokenization scheme you think performs the best.\n",
    "2. For each of the bigrams in that abstracts, compute the mutual information of that bigram\n",
    "3. Pick a subset of bigrams and and print their mutual information value to the notebook.\n",
    "4. Answer the questions in the free response section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pXgmepfDDAm"
   },
   "source": [
    "### Question 6A: Computing mutual information for bigrams in one sentence (10/15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXPXWtAfCRGy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information of Bigrams:\n",
      "[(('[', 'reads'), 14.234443239689758), (('acherontis', 'propitii'), 14.234443239689758), (('afric', 'shore'), 14.234443239689758), (('almain', 'rutters'), 14.234443239689758), (('ancient', 'gentlewoman'), 14.234443239689758), (('aquam', 'quam'), 14.234443239689758), (('aquatani', 'spiritus'), 14.234443239689758), (('ardentis', 'monarcha'), 14.234443239689758), ((\"as't\", 'passes'), 14.234443239689758), (('astronomy', 'graven'), 14.234443239689758)]\n"
     ]
    }
   ],
   "source": [
    "## your code for Question 6 goes here\n",
    "# 1. Compute the bigram frequencies of all words in our `faustus` corpus. You may use whatever tokenization scheme you think performs the best.\n",
    "# use the nltk bigram function to get the bigrams from the faustus corpus\n",
    "faust_bigrams = nltk.bigrams(faust_nltk_subset)\n",
    "\n",
    "# 2. For each of the bigrams in that abstracts, compute the mutual information of that bigram\n",
    "# this is the best i could find to compute \n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "bigram_finder = BigramCollocationFinder.from_words(faust_nltk_subset)\n",
    "mi = bigram_finder.score_ngrams(BigramAssocMeasures().pmi)\n",
    "\n",
    "# 3. Pick a subset of bigrams and and print their mutual information value to the notebook.\n",
    "print(\"Mutual Information of Bigrams:\")\n",
    "print(mi[:10])\n",
    "\n",
    "# 4. Answer the questions in the free response section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIEw6FcsdZ1f"
   },
   "source": [
    "### Question 6B: Free response (5/10 points)\n",
    "\n",
    "The highest mutual information values are for word pairs like \"afric shore\" and \"ancient gentlewoman,\" which shows that these words strongly depend on each other. These word pairs are rare but meaningful, so their relationship is more significant than common words like \"the\" or \"and.\" Lower values would be from words that are frequent but without any profound association. Mutual information is better than conditional probability because it shows important word relationships rather than just how often one word follows the other. Conditional probability is therefore useful in finding important phrases in a text and not just common ordinary word patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzshSnbtGMCo"
   },
   "source": [
    "### <font color='red'>Your written response for question 6B goes here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PCgDQrOaICRl"
   },
   "source": [
    "# Self Evaluation (5 points, added only if at least half of the previous exercises has been attempted)\n",
    "\n",
    "How do you think you did? Which parts did you struggle with the most? Which parts were easier than you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think I worked well at contrasting the tokenization methods and their results. It was simple to contrast differences among regular expressions, NLTK, and spaCy when looking at how they handled punctuation and contractions. The most difficult part was the interpretation of mutual information values and giving explanations for why certain pairs of words had high scores. More critical thinking was required than a simple comparison of token counts to know why these relations were present. On the other hand, identifying the biggest points of contention between tokenizers was easier than expected because the patterns stood out in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
