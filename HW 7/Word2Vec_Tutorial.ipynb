{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7dwyEKcCGpx"
   },
   "source": [
    "# Lab 6: A Quick Introduction to Word2Vec with Python and Gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPKosWFjCGp3"
   },
   "source": [
    "## Setting up and getting some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2XMULWTxCGp4"
   },
   "outputs": [],
   "source": [
    "# We need a resource for our data\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8am5-PssCGp5"
   },
   "outputs": [],
   "source": [
    "# If you are using this in CoLab, also run this cell, otherwise you can skip it\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e1_jRsY-CGp5",
    "outputId": "b3c6dc97-2063-4a4e-c24d-e18996ee90b0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Henry\n",
      "[nltk_data]     Pham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# We are going to use the brown corpus\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bg2Gf9byCGp6"
   },
   "source": [
    "Let's start by printing a few sentences out of the \"brown\" corpus, to get an idea of what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4lxxHqRrCGp6",
    "outputId": "a53a774f-84a6-4fb2-e0a2-5d013b9ff12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.']]\n"
     ]
    }
   ],
   "source": [
    "brown_sent = brown.sents()\n",
    "print(brown_sent[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yyZ3Y0XCGp6"
   },
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H__ioQy3CGp6"
   },
   "source": [
    "We don't want to build the whole model from scratch, we will use the Gensim library instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lPyvNxluCGp7"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaSwiWIyCGp7"
   },
   "source": [
    "We can now build an instance of the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cjF0VMYMCGp7"
   },
   "outputs": [],
   "source": [
    "# This is the whole model for the brown corpus (it might take a few minutes)!\n",
    "brown_model = Word2Vec(brown_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byGADIbKCGp7"
   },
   "source": [
    "Let's look at an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u0B20Qi5CGp9",
    "outputId": "bb467eb7-10bf-4a65-fa1a-20e32f90038b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'blue':\n",
      " [('gray', 0.9581364393234253), ('red', 0.9496833682060242), ('brown', 0.9451567530632019)]\n"
     ]
    }
   ],
   "source": [
    "test1 = brown_model.wv.most_similar('blue')\n",
    "print(\"Most similar to 'blue':\\n\", test1[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6eBbaerCGp9"
   },
   "source": [
    "## Refining the model\n",
    "\n",
    "Word2Vec takes a broad range of parameters. In our example above, we only chose where to get our sentences from, and we used the *default* settings for the rest. But let's now look at a few that are most relevant (you can find a full list here: https://radimrehurek.com/gensim/models/word2vec.html):\n",
    "\n",
    "- **size**: The dimensionality of our embeddings (i.e. the length of each word vector).\n",
    "- **window**: Which words are considered contexts of the target. The size of window affects the type of similarity captured in the embeddings.\n",
    "- **negative**: The number of negative samples (incorrect training-pair instances) that are drawn for each good.\n",
    "- **sg**: Training algorithm -- 1 for skip-gram; otherwise CBOW.\n",
    "- **min_count**: Ignores all words with total frequency lower than this.\n",
    "- **iter**: Number of iterations (epochs) over the corpus.\n",
    "\n",
    "So let's now train our model by explicitly setting some of these parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HGX3o-wTCGp9"
   },
   "outputs": [],
   "source": [
    "# This is the whole model (it's going to take a few minutes!)\n",
    "brown_model = Word2Vec(brown_sent, window = 5, negative = 5, sg = 1, min_count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "DwNbGX61CGp9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'blue':\n",
      " [('gray', 0.933104932308197), ('pale', 0.9232453107833862), ('pink', 0.9185042381286621)]\n"
     ]
    }
   ],
   "source": [
    "# We can do the same test as before\n",
    "test = brown_model.wv.most_similar('blue')\n",
    "print(\"Most similar to 'blue':\\n\", test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dx9JWuqlCGp9"
   },
   "source": [
    "## Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asLIKZ2nCGp-"
   },
   "source": [
    "We are going to rely on our own **human intuitions** to decide how well the model is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iCsUj8MHCGp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How similar is 'cup' to 'water':\n",
      " 0.71435404\n",
      "How similar is 'cup' to 'book':\n",
      " 0.22795838\n"
     ]
    }
   ],
   "source": [
    "sim = brown_model.wv.similarity(\"cup\", \"water\")\n",
    "print(\"How similar is 'cup' to 'water':\\n\", sim)\n",
    "\n",
    "sim = brown_model.wv.similarity(\"cup\", \"book\")\n",
    "print(\"How similar is 'cup' to 'book':\\n\", sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kT4bgd-HCGp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'child':\n",
      " [('artist', 0.8327049016952515), ('teacher', 0.8274621963500977), ('joy', 0.8111901879310608)]\n"
     ]
    }
   ],
   "source": [
    "brown_test = brown_model.wv.most_similar('child')\n",
    "print(\"Most similar to 'child':\\n\", brown_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VfTS2eGgCGp-"
   },
   "source": [
    "We can do more complex comparisons, but some results will be less intuitive than others!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2gfevG4FCGp-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'child' but dissimilar to 'person':\n",
      " [('your', 0.29096704721450806), ('living', 0.2397458702325821), ('health', 0.22753654420375824)]\n"
     ]
    }
   ],
   "source": [
    "brown_test = brown_model.wv.most_similar(positive = ['child'], negative = ['person'])\n",
    "print(\"Most similar to 'child' but dissimilar to 'person':\\n\", brown_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OepVBfzvCGp-"
   },
   "source": [
    "### Let's try a few more interesting tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tLlaRH0fCGp-"
   },
   "source": [
    "Which word is a mismatch in the sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "aoGzI77cCGp_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red\n"
     ]
    }
   ],
   "source": [
    "mismatch = brown_model.wv.doesnt_match(['teacher','professor','doctor','red','athlete','runner'])\n",
    "print(mismatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NadQXuRhCGp_"
   },
   "source": [
    "Maybe not **just** semantic relations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "T4AKRwb7CGp_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading\n"
     ]
    }
   ],
   "source": [
    "mismatch = brown_model.wv.doesnt_match(['running','swimming','singing','paper','reading','booking','catch'])\n",
    "print(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Nx54ozS6CGp_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between 'walk' and 'walked':\n",
      " 0.77256596\n",
      "The similarity between 'look' and 'looked':\n",
      " 0.76084566\n",
      "The similarity between 'look' and 'walk':\n",
      " 0.76290905\n"
     ]
    }
   ],
   "source": [
    "compare = brown_model.wv.similarity('walk','walked') \n",
    "print(\"The similarity between 'walk' and 'walked':\\n\", compare)\n",
    "\n",
    "compare = brown_model.wv.similarity('look','looked') \n",
    "print(\"The similarity between 'look' and 'looked':\\n\", compare)\n",
    "\n",
    "compare = brown_model.wv.similarity('look','walk') \n",
    "print(\"The similarity between 'look' and 'walk':\\n\", compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xaq0BKT7CGp_"
   },
   "source": [
    "## The choice of training data\n",
    "\n",
    "As for the other parameters that we looked at, the **choice of training data** (our corpus) is essential in driving model performance.\n",
    "For example, consider a very famous test case for Word2Vec: is the model able to derive the fact that \"woman\" is to \"queen\" what \"man\" is to \"king\"?\n",
    "\n",
    "We can represent this question algebraically as:\n",
    "\n",
    "$$vector(woman) +  vector(king) - vector(man) = vector(queen)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "DY0Zny_TCGp_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('singing', 0.8299245238304138)]\n"
     ]
    }
   ],
   "source": [
    "test = brown_model.wv.most_similar(positive=['woman','king'], negative=['man'], topn=1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aVPENvc9CGp_"
   },
   "source": [
    "We got a *weird* result!\n",
    "\n",
    "However, consider the fact that the brown corpus is not too big (1M words) and it is fairly old. What would happen if we used a bigger, more recent corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8qLaCJ_CGp_"
   },
   "source": [
    "### Working with a pretrained model\n",
    "\n",
    "Luckily, NLTK includes a pre-trained model. In particular, it includes part of a model trained on 100 billion words from the Google News Dataset. The full model is from https://code.google.com/p/word2vec/ (about 3 GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gPwOoc1UCGqA",
    "outputId": "e6e8e429-d69d-4340-8d97-361a5e4e584a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package word2vec_sample to C:\\Users\\Henry\n",
      "[nltk_data]     Pham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping models\\word2vec_sample.zip.\n"
     ]
    }
   ],
   "source": [
    "# we need to get the data\n",
    "from nltk.data import find\n",
    "nltk.download('word2vec_sample')\n",
    "\n",
    "# we are going to use a pruned set\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jlCWX7fUCGqA"
   },
   "outputs": [],
   "source": [
    "# This time we are **not** training it from scratch, we are just loading it in (it is still going to take a bit)!\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3BtG6NyCGqA"
   },
   "source": [
    "Let's do a sanity check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dkz91uJgCGqA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('red', 0.7225173115730286),\n",
       " ('purple', 0.7134224772453308),\n",
       " ('white', 0.6606029868125916)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"blue\")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD_zPjcNCGqA"
   },
   "source": [
    "Let's try our example once more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7aybydK8CGqA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman','king'], negative=['man'], topn = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9P044jWvCGqA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('France', 0.7884091734886169)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['Paris','Germany'], negative=['Berlin'], topn = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXPghiAHCGqA"
   },
   "source": [
    "We can do more! Let's track **semantic shifts** (e.g. historical changes in meaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "90HWMgI8CGqA",
    "outputId": "b29d0523-5f3f-4e90-997b-211571d2977a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'gay' in the brown corpus:\n",
      " [('lonely', 0.9122912883758545), ('Tommy', 0.906644880771637), ('unhappy', 0.900947630405426), ('awfully', 0.8988267779350281), ('gaiety', 0.8983256220817566)]\n"
     ]
    }
   ],
   "source": [
    "change1 = brown_model.wv.most_similar('gay')\n",
    "print(\"Most similar to 'gay' in the brown corpus:\\n\", change1[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jBUamVUlCGqB",
    "outputId": "8711c9f5-426b-47d4-97c6-82fc608a64b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to 'gay' in Google News:\n",
      " [('homosexual', 0.8145633935928345), ('homosexuals', 0.7562745213508606), ('lesbians', 0.7516927719116211), ('queer', 0.6972684264183044), ('Gay', 0.6740463376045227)]\n"
     ]
    }
   ],
   "source": [
    "change2 = model.most_similar('gay')\n",
    "print(\"Most similar to 'gay' in Google News:\\n\", change2[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GcRh-pMCGqB"
   },
   "source": [
    "## Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7u6pqtXCGqB"
   },
   "source": [
    "Relying on frequency patterns in human-generated data to make inferences has some problems..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bEwG3-5jCGqB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between 'she' and 'engineer':\n",
      " 0.0032564793\n",
      "The similarity between 'he' and 'engineer':\n",
      " 0.107617\n"
     ]
    }
   ],
   "source": [
    "compare1 = model.similarity('she','engineer')\n",
    "print(\"The similarity between 'she' and 'engineer':\\n\", compare1)\n",
    "\n",
    "compare2 = model.similarity('he','engineer')\n",
    "print(\"The similarity between 'he' and 'engineer':\\n\", compare2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "PUBMQ1AQCGqC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between 'woman' and 'nurse':\n",
      " 0.44135568\n",
      "The similarity between 'man' and 'nurse':\n",
      " 0.25472283\n"
     ]
    }
   ],
   "source": [
    "compare1 = model.similarity('woman','nurse')\n",
    "print(\"The similarity between 'woman' and 'nurse':\\n\", compare1)\n",
    "\n",
    "compare2 = model.similarity('man','nurse')\n",
    "print(\"The similarity between 'man' and 'nurse':\\n\", compare2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONr5nwU8CGqC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKPK5TQndLMH"
   },
   "source": [
    "**Exercise 1. (5 points)**\n",
    "\n",
    "Pick 2 words of your choice as use the code below to extract their 3 closest words from the brown semantic space and the google semantic space. Which model better captures your intuition? Sum up your considerations in a few sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EddjfpYadm05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown Corpus, most similar to 'people':\n",
      " [('Americans', 0.7912866473197937), ('Negroes', 0.7862326502799988), ('readers', 0.7780669331550598)]\n",
      "Google Corpus, most similar to 'people':\n",
      " [('individuals', 0.5827619433403015), ('folks', 0.5794458985328674), ('citizens', 0.5653229355812073)]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: I just realized I should haev used some gendered words but :p\n",
    "# Word 1\n",
    "# Brown modify this code with a word of your choice\n",
    "brown_test = brown_model.wv.most_similar('people')\n",
    "print(\"Brown Corpus, most similar to 'people':\\n\", brown_test[:3])\n",
    "\n",
    "\n",
    "\n",
    "#Google modify this code with words of your choice\n",
    "\n",
    "google_test = model.most_similar(\"people\")\n",
    "print(\"Google Corpus, most similar to 'people':\\n\", google_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "g2SYPG3kdvY-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brown Corpus, most similar to 'amazing':\n",
      " [('contradiction', 0.9285436868667603), ('uncertain', 0.9285029768943787), ('god', 0.925239622592926)]\n",
      "Google Corpus, most similar to 'amazing':\n",
      " [('incredible', 0.9054001569747925), ('awesome', 0.8282866477966309), ('unbelievable', 0.8201264142990112)]\n"
     ]
    }
   ],
   "source": [
    "# Word 2\n",
    "# Brown modify this code with a word of your choice\n",
    "\n",
    "brown_test = brown_model.wv.most_similar('amazing')\n",
    "print(\"Brown Corpus, most similar to 'amazing':\\n\", brown_test[:3])\n",
    "\n",
    "#Google modify this code with words of your choice\n",
    "\n",
    "google_test = model.most_similar(\"amazing\")\n",
    "print(\"Google Corpus, most similar to 'amazing':\\n\", google_test[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUdfOH9VefJT"
   },
   "source": [
    "*write your considerations for exercise 1 here*\n",
    "\n",
    "The Google model much better encapsualtes my intuition for both of the words I provided. For my first word, I would assume 'people' to be a unanimous term that doesn't have any specifities behind it yet the brown model added specifity and even just included 'Negroe' out of nowhere. When it comes to the second word, amazing, I would assume it is also a unanimous term though the Brown model tagged 'god' to it. Moreover the brown model tagged terms that are more related to something like 'unknown' instead of my intuitive defeinition of amazing which is 'cool', 'radical', or 'awesome'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2Cn2L7ucEbt"
   },
   "source": [
    "**Exercise 2. (5 points)**\n",
    "\n",
    "Think of two more cases of implicit biases that you can test the model on (they can be based on gender as above, but it would be even better if you could think of other dimensions for bias). Then, modify the code below by switching w1 and w2 with words of your choice to test your idea. Did the model output what you expected? Summarize your conclusions in a couple of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "hOlTm0-ecsGg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between 'money' and 'successful':\n",
      " 0.16267024\n",
      "The similarity between 'broke' and 'successful':\n",
      " 0.0012424086\n"
     ]
    }
   ],
   "source": [
    "#Bias example 1\n",
    "\n",
    "\n",
    "compare1 = model.similarity('money', 'successful')\n",
    "print(\"The similarity between 'money' and 'successful':\\n\", compare1)\n",
    "\n",
    "compare2 = model.similarity('broke', 'successful')\n",
    "print(\"The similarity between 'broke' and 'successful':\\n\", compare2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "gLVnhUXucwsa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between 'education' and 'job':\n",
      " 0.22259279\n",
      "The similarity between 'unemployed' and 'job':\n",
      " 0.4136857\n"
     ]
    }
   ],
   "source": [
    "#Bias example 2\n",
    "compare3 = model.similarity('education', 'job')\n",
    "print(\"The similarity between 'education' and 'job':\\n\", compare3)\n",
    "\n",
    "compare4 = model.similarity('unemployed', 'job')\n",
    "print(\"The similarity between 'unemployed' and 'job':\\n\", compare4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y--pfAxZcs3P"
   },
   "source": [
    "#Write your consideration here\n",
    "\n",
    "The model partly gave results I expected. It showed a strong connection between money and success, and almost none between broke and success, which matches common bias. But it surprisingly linked unemployed to job more than education to job. This suggests the model sometimes reflects how words are used together, not just real-world values or assumptions. It reminds us that language models learn from patterns in text, not from understanding the meaning like humans do. So while they can reflect real biases, they can also behave in unexpected ways based on word usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Word2Vec_Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
