Henry Phạm   
Professor Rawski  
LING 165 \- Intro to Natural Language Processing  
May 14th, 2025  
Text Normalization for Speech or Chatbots Using Finite-State Transducers  
Language is constantly evolving, and in the internet age, there are few forces of change that occur as rapidly as on the internet itself and among youth cultures. Gen Z slang, particularly, is a frontier of informal communication where normal grammar, vocabulary, and syntax are typically abandoned for acronyms, emojis, phonetic spellings, and cultural references. Since linguists are concerned with this shift, this is also a challenging task for natural language processing (NLP) systems typically trained on typical text corpora. In order to process inputs effectively ranging from virtual assistants and chatbots to speech recognition and content filtering, these systems first have to normalize nonstandard language into standard English. This project explored the computational approach to this problem through Finite-State Transducers (FSTs), a deterministic and symbolic string-to-string transformation method founded on rule-based grammars.  
The goal of the project was to create a system that would identify Gen Z slang in natural sentences and replace such words with their standard English equivalents. For example, if the input "that's cap fr fr" should be translated to "that's a lie for real for real." To ensure transparency, modularity, and efficiency, we aimed to use Pynini, a Python library for compiling and concatenating FSTs from OpenFST. Compared to deep learning models, FSTs offer an explainable and controllable mechanism of language transformation properties that are of particular value in filtering, preprocessing, or post-processing actions in NLP pipelines.  
The foundation of the project was the Hugging Face Gen Z Slang Dataset that contains slang terms and definitions. The dataset allowed us to create mappings from standard English to slang. These mappings were to be translated into rewrite rules with the help of Pynini and eventually merged into a unified transducer. The project was planned to progress in stages: slang word detection, building word-level transducers, extending to sentence-level transformations, and evaluating the system's accuracy.  
However, despite planning, technical constraints made complete implementation impossible. The largest problem was environment-based, that of installing Pynini and its C++ counterpart, OpenFST. The libraries contain Unix-like environments or specialized step-by-step configuration routines that are inadequately supported under Windows or some Jupyter-based platforms. I first attempted to install Pynini on my own machine but found it very difficult to get the required C++ files set up and it actually compromised other functionalities of my system. Time soon took on an enormous barrier status, spent hours on these installation issues instead of further work on model training. I then switched over to Google Colab so I could work within a cleaner, Unix-based environment. As a result, while the data set was explored and a notebook format was developed, no model could be trained or tested. However, significant effort was put into building out the architecture of the transducer system and experimenting with initial code.  
In natural language processing (NLP), one such long-standing issue is handling slang or colloquial language, which can vary significantly from standard grammar and vocabulary. This project tried to develop a system that was able to recognize slang words in a sentence and translate them into their standard English equivalents. The solution was based on the use of Finite-State Transducers (FSTs), a formal method for string transformations' representation, coupled with the Pynini library, a Python API for constructing and using such transducers. The aim was to develop an FST to operate at the sentence level, processing input strings, identifying slang words, and replacing them with their formal representations while preserving sentence structure and readability.  
Essentially, the system is based on a function named slang\_to\_english that uses a pre-defined FST (slang\_fst) to translate words from slang to their normal English counterparts. The simplest method of using the mapping is word-by-word translation of each word in a sentence. The function transform\_sentence\_simple(sentence) tokenizes the sentence into words, uses the FST on each word separately, and reconstructs the translated words into an entire sentence. While this straightforward method is adequate for straightforward cases, it does not account for much of the subtlety of actual language, like punctuation symbols, compound offensives that consist of more than one word, or capitalization differences.  
These challenges were not forgotten, however, during the initial proposals for the project. The initial proposals to build the FST were actually attempting to address issues like punctuation, whitespace, and multi-word slang. Identity mappings were employed to allow for non-slang tokens to be preserved, and playing around with Pynini constructs pn.union() and pn.closure() was undertaken to allow for flexible sequencing and pattern matching. Specifically, one of the early FST implementations made an attempt at composing the slang-replacement transducer through identity mappings so that the remainder of the sentence would just pass through unchanged, an attempt at building a more complete, compositional system. Furthermore, initial experimentation with code indicated methods of addressing word boundaries and spacing directly by using middle transducers to identify space and punctuation. While such early FST constructions were theoretically in line with the final objective, effective and proper implementation thereof turned out to be far more complex than initially anticipated.  
As the project progressed, it became clear that a fully robust sentence-level FST would require even more intricate design, one capable of combining multiple transducers, managing whitespace and punctuation, and recognizing complex slang patterns. In light of these complexities, a more practical and intermediate approach was developed using Pynini’s rewrite.one\_top\_rewrite() method. This function enabled runtime application of the compiled FST to individual tokens in the sentence. By lowercasing every word before transforming it, and catching rewrite errors by exception handling, the system was able to avoid crashes when encountering unknown words. If a slang transformation was recognized for the word, the function performed it, if not, the word was left unchanged. This strategy was much of an improvement from the initial strategy since it introduced robustness by means of error tolerance and case-insensitive matching. But it still shared most of the same weaknesses. In particular, it didn't identify and translate multi-word slang expressions, ignored punctuation entirely, and couldn't cope with slang that was present within subwords, such as hashtags or emojis.  
Nevertheless, the underlying FST reasoning that was built in the initial work on the project, namely the effort to integrate identity and replacement mappings, and to cope with spaces and edges, remains valid and is applicable.  While the prototype itself could not breach the shift into a full sentence-level model, the basis laid in these tests gives clear feedback for subsequent improvements. Despite these limitations, the project laid a solid conceptual and technical foundation. The planned direction was to use context-dependent rewrite rules via pynini.cdrewrite() for more robust matching. For example, detecting “he capping fr” and rewriting it to “he is lying for real” would require both lexical understanding and boundary awareness, something achievable with Pynini’s composition capabilities.  
Looking ahead, there are several clear opportunities for expanding and enhancing the current system. One possible line of approach is to add weights and ranking to the finite-state transducer. By assigning probabilities or frequencies of use to slang definitions, the model would be more capable of handling ambiguity. For example, a term like “dead” can mean a range of things, from literal death to expressions of emotional intensity or disbelief, such as “I’m dead” meaning “I’m laughing a lot.” Without context, it's difficult for a model to select the correct interpretation. By adding weighted alternatives, the transducer would be capable of preferencing the most likely meaning according to patterns of use in context or training data frequency, improving responsiveness and accuracy in real-world applications.  
Another approach is developing a graphical user interface (GUI) or web app. Such an interface would make the system more user-friendly to users from non-technical backgrounds, such as teachers, linguists, or just general members of the public interested in the use of contemporary slang. With a non-technical area for inputting sentences and what they translate to, the tool could both be of use in practice and for teaching purposes. It could help parents understand their children's online vocabulary better, assist educators in deciphering students' vocabularies, or just give users an engaging experience to discover emerging language trends.  
Lastly, a finalized version of the transducer system could be integrated into more extensive natural language processing (NLP) pipelines. For instance, it might enhance speech recognition systems by converting unstructured spoken language to more formalized transcriptions. It could also serve a useful role in chatbot development in enabling bots to respond more naturally to slang-filled input or translating output for users who are not used to such language. Additionally, in data preprocessing and cleaning for such applications as sentiment analysis, social media monitoring, or customer feedback systems, the transducer may normalize noisy or informal text, resulting in more consistent and stable analysis. These potential extensions serve to illustrate the applicability and utility of the FST-based approach and show how even a prototype system, if expanded, can become a crucial component of many practical NLP applications.  
The consequences of this work are theoretical as well. In a world in which end-to-end machine learning systems are increasingly hegemonic, symbolic systems like FSTs remind us of the importance of human-readable, interpretable, and controllable models. FSTs particularly shine in those uses where deterministic behavior and linguistic insight are more valuable than brute-force pattern recognition. In fact, products like Google's speech-to-text and Apple's Siri still employ FSTs in production, especially in multilingual and domain-specific scenarios where control and speed are critical.  
In conclusion, this project explored the daunting but interesting task of normalizing Gen Z slang using the principles of rule-based grammars and finite-state transducers. While the project was not able to present a working implementation due to environment constraints, it was in a position to present the structure, justification, and process necessary for such a system. All scaffolding necessary to build upon when the technical issues are smoothed out is presented in the Jupyter Notebook. With further development, this work can lead to practical applications in accessibility, education, and commercial NLP services, in addition to furthering an understanding of language variation in the digital age.
